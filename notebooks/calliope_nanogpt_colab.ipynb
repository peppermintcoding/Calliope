{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZpyla00mnCd"
      },
      "source": [
        "### Calliope - POETRY LLM\n",
        "\n",
        "[Calliope Repository](https://github.com/peppermintcoding/Calliope)\n",
        "\n",
        "    Here rise to life again, dead poetry!\n",
        "    Let it, O holy Muses, for I am yours,\n",
        "    And here Calliope, strike a higher key,\n",
        "    Accompanying my song with that sweet air\n",
        "    which made the wretched Magpies feel a blow\n",
        "    that turned all hope of pardon to despair\n",
        "Dante, \"Purgatorio\", Canto I, lines 7 to 12\n",
        "\n",
        "[Calliope Wikipedia](https://en.wikipedia.org/wiki/Calliope)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiPuMLkjmnCg"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/peppermintcoding/Calliope.git\n",
        "# !pip install -q -r \"Calliope/requirements.txt\"\n",
        "!pip install -q tiktoken # on colab only need tiktoken\n",
        "# set libcuda.so for torch.compile\n",
        "!ldconfig /usr/lib64-nvidia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euZ0TIfum0Py"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCy9MUT6mnCh"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = (16, 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaMgZ1zFmnCi"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFSfuLv9mnCj"
      },
      "outputs": [],
      "source": [
        "# poor man's data loader\n",
        "train_data = np.memmap('../gdrive/MyDrive/Calliope/train.npy', dtype=np.uint16, mode='r')\n",
        "print(f\"Number of trainings token: {len(train_data):,}\")\n",
        "def get_batch():\n",
        "    ix = torch.randint(len(train_data) - model_args[\"block_size\"], (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((train_data[i:i+model_args[\"block_size\"]]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((train_data[i+1:i+1+model_args[\"block_size\"]]).astype(np.int64)) for i in ix])\n",
        "    if device == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af9z3jKVmnCj"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# Calliope-150m -> batch_size: 8\n",
        "model_args = {\n",
        "    \"n_layer\": 16,\n",
        "    \"n_head\": 24,\n",
        "    \"n_embd\": 768,\n",
        "    \"dropout\": 0.1,\n",
        "    \"bias\": False,\n",
        "    \"block_size\": 1024,\n",
        "}\n",
        "# Calliope-67m -> batch_size: 24\n",
        "model_args = {\n",
        "    \"n_layer\": 4,\n",
        "    \"n_head\": 16,\n",
        "    \"n_embd\": 768,\n",
        "    \"dropout\": 0.1,\n",
        "    \"bias\": False,\n",
        "    \"block_size\": 512,\n",
        "}\n",
        "# Calliope-250m -> batch_size: 16\n",
        "model_args = {\n",
        "    \"n_layer\": 16,\n",
        "    \"n_head\": 32,\n",
        "    \"n_embd\": 1024,\n",
        "    \"dropout\": 0.1,\n",
        "    \"bias\": False,\n",
        "    \"block_size\": 512,\n",
        "}\n",
        "\"\"\"\n",
        "model_args = {\n",
        "    \"n_layer\": 16,\n",
        "    \"n_head\": 32,\n",
        "    \"n_embd\": 1024,\n",
        "    \"dropout\": 0.1,\n",
        "    \"bias\": False,\n",
        "    \"block_size\": 512,\n",
        "}\n",
        "\n",
        "out_dir = 'out-poetry'\n",
        "checkpoint_interval = 500 # save checkpoint every n steps\n",
        "log_interval = 100 # sync cpu and gpu not too often\n",
        "\n",
        "gradient_accumulation_steps = 1\n",
        "batch_size = 16\n",
        "\n",
        "epochs = 2\n",
        "max_iters = int(train_data.shape[0] / (gradient_accumulation_steps * batch_size * model_args[\"block_size\"]) * epochs)\n",
        "print(f\"Max Iterations: {max_iters}\")\n",
        "\n",
        "learning_rate = 4e-4\n",
        "decay_lr = True # whether to decay the learning rate\n",
        "lr_decay_iters = 0.95*max_iters # should be ~= max_iters per Chinchilla\n",
        "min_lr = 4e-5 # should be ~= learning_rate/10 per Chinchilla\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95 # make a bit bigger because number of tokens per iter is small\n",
        "warmup_iters = 2_000 # how many steps to warm up for\n",
        "\n",
        "# adamw optimizer\n",
        "weight_decay = 1e-1\n",
        "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
        "\n",
        "# system\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "print(f\"training in dytpe: {dtype}\")\n",
        "compile = True # use PyTorch 2.0 to compile the model to be faster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRl2E3SDmnCj"
      },
      "outputs": [],
      "source": [
        "from Calliope.model import GPTConfig, GPT\n",
        "\n",
        "seed_offset = 0\n",
        "tokens_per_iter = gradient_accumulation_steps * batch_size * model_args[\"block_size\"]\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "torch.manual_seed(69 + seed_offset)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "# note: float16 data type will automatically use a GradScaler\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "\n",
        "gptconf = GPTConfig(**model_args)\n",
        "model = GPT(gptconf).to(device)\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device)\n",
        "\n",
        "if compile:\n",
        "    print(\"compiling the model... (takes a ~minute)\")\n",
        "    model = torch.compile(model)\n",
        "\n",
        "# learning rate decay scheduler (cosine with warmup)\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iAuY3nELmnCk"
      },
      "outputs": [],
      "source": [
        "X, Y = get_batch()\n",
        "train_loss_history = []\n",
        "\n",
        "pbar = tqdm(range(max_iters+1))\n",
        "for iter_num in pbar:\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # write checkpoints\n",
        "    if iter_num % checkpoint_interval == 0 or iter_num == max_iters:\n",
        "        if iter_num > 0:\n",
        "            checkpoint = {\n",
        "                'model': model.state_dict(),\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'model_args': model_args,\n",
        "                'iter_num': iter_num,\n",
        "            }\n",
        "            torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "\n",
        "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "    # and using the GradScaler if data type is float16\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        with torch.amp.autocast(device_type=device, dtype=ptdtype):\n",
        "            logits, loss = model(X, Y)\n",
        "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
        "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "        X, Y = get_batch()\n",
        "        scaler.scale(loss).backward()\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    if iter_num % log_interval == 0:\n",
        "        # get loss as float. note: this is a CPU-GPU sync point\n",
        "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        train_loss_history.append([iter_num, lossf])\n",
        "        pbar.set_description(f\"step {iter_num:,}: loss {lossf:.4f}: lr {lr:.6f}\")\n",
        "    if np.isnan(lossf):\n",
        "        print(f\"Loss nan at {iter_num} iter steps.\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eQusaTMCmnCk"
      },
      "outputs": [],
      "source": [
        "plt.plot([x[0] for x in train_loss_history], [x[1] for x in train_loss_history], c=\"r\", label=\"train loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# upload checkpoint to drive\n",
        "checkpoint = {\n",
        "    'model': model.state_dict(),\n",
        "    'optimizer': optimizer.state_dict(),\n",
        "    'model_args': model_args,\n",
        "    'iter_num': max_iters,\n",
        "}\n",
        "torch.save(checkpoint, \"../gdrive/MyDrive/Calliope/Checkpoints/ckpt.pt\")"
      ],
      "metadata": {
        "id": "RiJRdh4eEREy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDE_YM_4mnCk"
      },
      "source": [
        "### Load Model from memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXn8AV-QmnCk"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "# checkpoint = torch.load(\"out-poetry/ckpt.pt\")\n",
        "checkpoint = torch.load(\"../gdrive/MyDrive/Calliope/Checkpoints/ckpt.pt\")\n",
        "gptconf = GPTConfig(**checkpoint[\"model_args\"])\n",
        "model = GPT(gptconf)\n",
        "\n",
        "# rename keys because of torch 2.1\n",
        "state_dict = {}\n",
        "for key, val in checkpoint[\"model\"].items():\n",
        "    if key.startswith(\"_orig_mod\"):\n",
        "        state_dict[key[10:]] = val\n",
        "    else:\n",
        "        state_dict[key] = val\n",
        "model.load_state_dict(state_dict)\n",
        "model.to(device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lxtVSdfmnCl"
      },
      "outputs": [],
      "source": [
        "prompt = \"I have not seen you in so long my dear\\n\"\n",
        "idx = model.generate(torch.tensor([tokenizer.encode(prompt)], device=device), 128, temperature=0.95)\n",
        "words = tokenizer.decode(idx[0].cpu().numpy())\n",
        "print(words)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 106396,
          "sourceId": 269207,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 3609428,
          "sourceId": 6277951,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4209665,
          "sourceId": 7263221,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4259507,
          "sourceId": 7355430,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4304757,
          "sourceId": 7409408,
          "sourceType": "datasetVersion"
        }
      ],
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}